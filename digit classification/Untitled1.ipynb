{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess data\n",
        "x_train = x_train.reshape(-1, 28*28) / 255.0\n",
        "x_test = x_test.reshape(-1, 28*28) / 255.0\n",
        "\n",
        "def one_hot(y, num_classes=10):\n",
        "    return np.eye(num_classes)[y]\n",
        "\n",
        "y_train_onehot = one_hot(y_train)\n",
        "y_test_onehot = one_hot(y_test)\n",
        "\n",
        "\n",
        "input_size = 784\n",
        "hidden1_size = 512\n",
        "hidden2_size = 256\n",
        "output_size = 10\n",
        "lambda_reg = 0.001\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "W1 = np.random.randn(input_size, hidden1_size) * np.sqrt(2. / input_size)\n",
        "b1 = np.zeros(hidden1_size)\n",
        "W2 = np.random.randn(hidden1_size, hidden2_size) * np.sqrt(2. / hidden1_size)\n",
        "b2 = np.zeros(hidden2_size)\n",
        "W3 = np.random.randn(hidden2_size, output_size) * np.sqrt(2. / hidden2_size)\n",
        "b3 = np.zeros(output_size)\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def softmax(x):\n",
        "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "# Forward propagation\n",
        "def forward(x):\n",
        "    z1 = np.dot(x, W1) + b1\n",
        "    a1 = relu(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = relu(z2)\n",
        "    z3 = np.dot(a2, W3) + b3\n",
        "    a3 = softmax(z3)\n",
        "    return z1, a1, z2, a2, z3, a3\n",
        "\n",
        "\n",
        "def compute_loss(y_true, a3):\n",
        "    m = y_true.shape[0]\n",
        "    log_likelihood = -np.log(a3[range(m), np.argmax(y_true, axis=1)] + 1e-10)\n",
        "    loss = np.sum(log_likelihood) / m\n",
        "    reg_term = (lambda_reg/(2*m)) * (np.sum(W1**2) + np.sum(W2**2) + np.sum(W3**2))\n",
        "    return loss + reg_term\n",
        "\n",
        "# Backward propagation\n",
        "def backward(x, y_true, z1, a1, z2, a2, a3):\n",
        "    m = x.shape[0]\n",
        "\n",
        "    # Output layer gradient\n",
        "    dz3 = (a3 - y_true) / m\n",
        "    dw3 = np.dot(a2.T, dz3) + (lambda_reg/m) * W3\n",
        "    db3 = np.sum(dz3, axis=0)\n",
        "\n",
        "    # Hidden layer 2\n",
        "    da2 = np.dot(dz3, W3.T)\n",
        "    dz2 = da2 * (z2 > 0)\n",
        "    dw2 = np.dot(a1.T, dz2) + (lambda_reg/m) * W2\n",
        "    db2 = np.sum(dz2, axis=0)\n",
        "\n",
        "    # Hidden layer 1\n",
        "    da1 = np.dot(dz2, W2.T)\n",
        "    dz1 = da1 * (z1 > 0)\n",
        "    dw1 = np.dot(x.T, dz1) + (lambda_reg/m) * W1\n",
        "    db1 = np.sum(dz1, axis=0)\n",
        "\n",
        "    return dw1, db1, dw2, db2, dw3, db3\n",
        "\n",
        "\n",
        "epochs = 50\n",
        "batch_size = 128\n",
        "learning_rate = 0.01\n",
        "\n",
        "\n",
        "n_samples = x_train.shape[0]\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle data\n",
        "    permutation = np.random.permutation(n_samples)\n",
        "    x_train_shuffled = x_train[permutation]\n",
        "    y_train_shuffled = y_train_onehot[permutation]\n",
        "\n",
        "    # Mini-batch gradient descent\n",
        "    for i in range(0, n_samples, batch_size):\n",
        "        x_batch = x_train_shuffled[i:i+batch_size]\n",
        "        y_batch = y_train_shuffled[i:i+batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        z1, a1, z2, a2, z3, a3 = forward(x_batch)\n",
        "\n",
        "        # Backward pass\n",
        "        dw1, db1, dw2, db2, dw3, db3 = backward(x_batch, y_batch, z1, a1, z2, a2, a3)\n",
        "\n",
        "        # Update parameters\n",
        "        W1 -= learning_rate * dw1\n",
        "        b1 -= learning_rate * db1\n",
        "        W2 -= learning_rate * dw2\n",
        "        b2 -= learning_rate * db2\n",
        "        W3 -= learning_rate * dw3\n",
        "        b3 -= learning_rate * db3\n",
        "\n",
        "    _, _, _, _, _, a3_train = forward(x_train)\n",
        "    train_loss = compute_loss(y_train_onehot, a3_train)\n",
        "    train_preds = np.argmax(a3_train, axis=1)\n",
        "    train_acc = np.mean(train_preds == y_train)\n",
        "\n",
        "    _, _, _, _, _, a3_test = forward(x_test)\n",
        "    test_preds = np.argmax(a3_test, axis=1)\n",
        "    test_acc = np.mean(test_preds == y_test)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\\n\")\n",
        "\n",
        "# Final evaluation\n",
        "_, _, _, _, _, a3_test = forward(x_test)\n",
        "test_preds = np.argmax(a3_test, axis=1)\n",
        "final_acc = np.mean(test_preds == y_test)\n",
        "print(f\"Final Test Accuracy: {final_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8hRY8ntGpwKI",
        "outputId": "f9928d1d-40e9-416c-8361-b1db7c08f251"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "Train Loss: 0.5070, Accuracy: 0.8701\n",
            "Test Accuracy: 0.8789\n",
            "\n",
            "Epoch 2/50\n",
            "Train Loss: 0.3689, Accuracy: 0.8994\n",
            "Test Accuracy: 0.9047\n",
            "\n",
            "Epoch 3/50\n",
            "Train Loss: 0.3182, Accuracy: 0.9116\n",
            "Test Accuracy: 0.9146\n",
            "\n",
            "Epoch 4/50\n",
            "Train Loss: 0.2872, Accuracy: 0.9192\n",
            "Test Accuracy: 0.9222\n",
            "\n",
            "Epoch 5/50\n",
            "Train Loss: 0.2666, Accuracy: 0.9247\n",
            "Test Accuracy: 0.9271\n",
            "\n",
            "Epoch 6/50\n",
            "Train Loss: 0.2496, Accuracy: 0.9306\n",
            "Test Accuracy: 0.9320\n",
            "\n",
            "Epoch 7/50\n",
            "Train Loss: 0.2355, Accuracy: 0.9340\n",
            "Test Accuracy: 0.9345\n",
            "\n",
            "Epoch 8/50\n",
            "Train Loss: 0.2219, Accuracy: 0.9379\n",
            "Test Accuracy: 0.9364\n",
            "\n",
            "Epoch 9/50\n",
            "Train Loss: 0.2107, Accuracy: 0.9415\n",
            "Test Accuracy: 0.9413\n",
            "\n",
            "Epoch 10/50\n",
            "Train Loss: 0.2010, Accuracy: 0.9435\n",
            "Test Accuracy: 0.9427\n",
            "\n",
            "Epoch 11/50\n",
            "Train Loss: 0.1922, Accuracy: 0.9458\n",
            "Test Accuracy: 0.9444\n",
            "\n",
            "Epoch 12/50\n",
            "Train Loss: 0.1846, Accuracy: 0.9479\n",
            "Test Accuracy: 0.9452\n",
            "\n",
            "Epoch 13/50\n",
            "Train Loss: 0.1764, Accuracy: 0.9504\n",
            "Test Accuracy: 0.9473\n",
            "\n",
            "Epoch 14/50\n",
            "Train Loss: 0.1700, Accuracy: 0.9526\n",
            "Test Accuracy: 0.9486\n",
            "\n",
            "Epoch 15/50\n",
            "Train Loss: 0.1628, Accuracy: 0.9540\n",
            "Test Accuracy: 0.9514\n",
            "\n",
            "Epoch 16/50\n",
            "Train Loss: 0.1577, Accuracy: 0.9558\n",
            "Test Accuracy: 0.9526\n",
            "\n",
            "Epoch 17/50\n",
            "Train Loss: 0.1530, Accuracy: 0.9568\n",
            "Test Accuracy: 0.9527\n",
            "\n",
            "Epoch 18/50\n",
            "Train Loss: 0.1465, Accuracy: 0.9588\n",
            "Test Accuracy: 0.9548\n",
            "\n",
            "Epoch 19/50\n",
            "Train Loss: 0.1414, Accuracy: 0.9601\n",
            "Test Accuracy: 0.9555\n",
            "\n",
            "Epoch 20/50\n",
            "Train Loss: 0.1376, Accuracy: 0.9615\n",
            "Test Accuracy: 0.9569\n",
            "\n",
            "Epoch 21/50\n",
            "Train Loss: 0.1320, Accuracy: 0.9628\n",
            "Test Accuracy: 0.9583\n",
            "\n",
            "Epoch 22/50\n",
            "Train Loss: 0.1282, Accuracy: 0.9639\n",
            "Test Accuracy: 0.9604\n",
            "\n",
            "Epoch 23/50\n",
            "Train Loss: 0.1241, Accuracy: 0.9656\n",
            "Test Accuracy: 0.9608\n",
            "\n",
            "Epoch 24/50\n",
            "Train Loss: 0.1203, Accuracy: 0.9663\n",
            "Test Accuracy: 0.9618\n",
            "\n",
            "Epoch 25/50\n",
            "Train Loss: 0.1170, Accuracy: 0.9673\n",
            "Test Accuracy: 0.9624\n",
            "\n",
            "Epoch 26/50\n",
            "Train Loss: 0.1135, Accuracy: 0.9685\n",
            "Test Accuracy: 0.9629\n",
            "\n",
            "Epoch 27/50\n",
            "Train Loss: 0.1111, Accuracy: 0.9692\n",
            "Test Accuracy: 0.9645\n",
            "\n",
            "Epoch 28/50\n",
            "Train Loss: 0.1073, Accuracy: 0.9708\n",
            "Test Accuracy: 0.9655\n",
            "\n",
            "Epoch 29/50\n",
            "Train Loss: 0.1043, Accuracy: 0.9713\n",
            "Test Accuracy: 0.9658\n",
            "\n",
            "Epoch 30/50\n",
            "Train Loss: 0.1033, Accuracy: 0.9713\n",
            "Test Accuracy: 0.9654\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-c98780c1ebc2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mdw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdw2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdw3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-c98780c1ebc2>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(x, y_true, z1, a1, z2, a2, a3)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mda1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdz2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mdz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mda1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mdw1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdz1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlambda_reg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0mdb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1_346neNuzAn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}